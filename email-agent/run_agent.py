from textwrap import dedent
from litellm import acompletion
from rich import print
import litellm
from dotenv import load_dotenv
import json
from dataclasses import asdict

from litellm.caching.caching import LiteLLMCacheType, Cache
from email_search_tools import search_emails, read_email
from langchain_core.utils.function_calling import convert_to_openai_tool
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt
from textwrap import dedent
from project_types import Scenario
import weave

litellm.cache = Cache(type=LiteLLMCacheType.DISK)

load_dotenv()

MAX_TURNS = 10

weave.init(project_name="agent-class-art")


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


@retry(stop=stop_after_attempt(3))
async def judge_correctness(
    scenario: Scenario, answer: str
) -> CorrectnessJudgeResponse:
    system_prompt = dedent(
        """
        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.
        """
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"
                f"Reference answer: {scenario.answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model="openai/gpt-4.1",
        messages=messages,
        caching=True,
        response_format=CorrectnessJudgeResponse,
    )

    first_choice = response.choices[0]  # type: ignore[attr-defined]
    raw_content = first_choice.message.content or "{}"  # type: ignore[attr-defined]

    try:
        return CorrectnessJudgeResponse.model_validate_json(raw_content)
    except Exception as e:
        # If parsing fails, fall back to 'accept': False
        return CorrectnessJudgeResponse(
            reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
        )


class FinalAnswer(BaseModel):
    answer: str
    source_ids: list[str]


async def run_agent(scenario: Scenario) -> FinalAnswer | None:
    system_prompt = dedent(
        """
        You are an email search assistant. You can use the tools provided to answer the user's question.
        """
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]

    def search_inbox(keywords: list[str]) -> list[dict]:
        """Search the inbox for emails matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_emails(
            inbox=scenario.inbox_address,
            keywords=keywords,
            sent_before=scenario.query_date,
        )
        # Convert each SearchResult dataclass instance to a plain dict
        return [asdict(result) for result in results]

    def return_final_answer(
        answer: str, reference_message_ids: list[str]
    ) -> FinalAnswer:
        return FinalAnswer(answer=answer, source_ids=reference_message_ids)

    tools = [search_inbox, read_email, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}

    for turn in range(MAX_TURNS):
        response = await acompletion(
            model="openai/gpt-4.1-mini",
            messages=messages,
            caching=True,
            tools=[convert_to_openai_tool(t) for t in tools_by_name.values()],
        )

        response_message = response.choices[0].message  # type: ignore[attr-defined]
        messages.append(response_message.model_dump(exclude_none=True))

        if response_message.content:
            return None

        if not response_message.tool_calls:
            return None

        for tool_call in response_message.tool_calls:
            tool_name: str = tool_call.function.name  # type: ignore
            if tool_name in tools_by_name:
                tool_args = json.loads(tool_call.function.arguments)
                tool_to_call = tools_by_name[tool_name]
                result = tool_to_call(**tool_args)
                if tool_name == "return_final_answer":
                    return result

                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": tool_name,
                        "content": str(result),
                    }
                )

    return None


@weave.op()
async def run_and_score_agent(scenario: Scenario) -> tuple[FinalAnswer | None, float]:
    answer = await run_agent(scenario)
    if answer is None:
        return None, 0.0
    correctness_judge_response = await judge_correctness(scenario, answer.answer)
    return answer, correctness_judge_response.accept


if __name__ == "__main__":
    import asyncio
    from load_scenarios import load_scenarios

    scenario = load_scenarios(limit=1)[0]
    answer = asyncio.run(run_and_score_agent(scenario))
    print(answer)
