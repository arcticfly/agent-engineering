import art
from textwrap import dedent
from litellm import acompletion
from rich import print
import litellm
from dotenv import load_dotenv
import json
from dataclasses import asdict

from litellm.caching.caching import LiteLLMCacheType, Cache
from email_search_tools import search_emails, read_email
from langchain_core.utils.function_calling import convert_to_openai_tool
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt
from textwrap import dedent
from project_types import Scenario
import weave
from art.utils.litellm import convert_litellm_choice_to_openai

litellm.cache = Cache(type=LiteLLMCacheType.DISK)

load_dotenv()

MAX_TURNS = 10

weave.init(project_name="agent-class-art-tmp")


class CorrectnessJudgeResponse(BaseModel):
    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(description="Whether the AI answer should be accepted.")


@retry(stop=stop_after_attempt(3))
async def judge_correctness(
    scenario: Scenario, answer: str
) -> CorrectnessJudgeResponse:
    system_prompt = dedent(
        """
        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).

        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.
        """
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"
                f"Reference answer: {scenario.answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model="openai/gpt-4.1",
        messages=messages,
        caching=True,
        response_format=CorrectnessJudgeResponse,
    )

    first_choice = response.choices[0]  # type: ignore[attr-defined]
    raw_content = first_choice.message.content or "{}"  # type: ignore[attr-defined]

    try:
        return CorrectnessJudgeResponse.model_validate_json(raw_content)
    except Exception as e:
        # If parsing fails, fall back to 'accept': False
        return CorrectnessJudgeResponse(
            reasoning=f"Parse error: {e}\nRaw: {raw_content}", accept=False
        )


class FinalAnswer(BaseModel):
    answer: str
    source_ids: list[str]


class ProjectTrajectory(art.Trajectory):
    final_answer: FinalAnswer | None = None


async def run_agent(model: art.Model, scenario: Scenario) -> ProjectTrajectory:
    traj = ProjectTrajectory(
        reward=0.0,
        messages_and_choices=[],
    )

    system_prompt = dedent(
        """
        You are an email search assistant. You can use the tools provided to answer the user's question.
        """
    )

    traj.messages_and_choices = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario.question},
    ]

    def search_inbox(keywords: list[str]) -> list[dict]:
        """Search the inbox for emails matching the given keywords and return
        a list of dictionaries so the LLM can easily consume them."""
        results = search_emails(
            inbox=scenario.inbox_address,
            keywords=keywords,
            sent_before=scenario.query_date,
        )
        # Convert each SearchResult dataclass instance to a plain dict
        return [asdict(result) for result in results]

    def return_final_answer(
        answer: str, reference_message_ids: list[str]
    ) -> FinalAnswer:
        """Return the final answer and the message IDs of the emails that were used to generate the answer."""
        return FinalAnswer(answer=answer, source_ids=reference_message_ids)

    tools = [search_inbox, read_email, return_final_answer]
    tools_by_name = {t.__name__: t for t in tools}

    if model.trainable:
        litellm_model_name = f"hosted_vllm/{model.name}"
    else:
        litellm_model_name = model.name

    for turn in range(MAX_TURNS):
        response = await acompletion(
            model=litellm_model_name,
            base_url=model.inference_base_url,
            api_key=model.inference_api_key,
            temperature=1,
            messages=traj.messages(),
            caching=not model.trainable,
            tools=[convert_to_openai_tool(t) for t in tools_by_name.values()],
        )

        response_message = response.choices[0].message  # type: ignore[attr-defined]
        traj.messages_and_choices.append(
            convert_litellm_choice_to_openai(response.choices[0])  # type: ignore[attr-defined]
        )  # type: ignore[attr-defined]

        if response_message.content or not response_message.tool_calls:
            # We always want tool calls. If they're missing, the model isn't
            # behaving how we want and we should just return the trajectory.
            return traj

        try:
            for tool_call in response_message.tool_calls:
                tool_name: str = tool_call.function.name  # type: ignore
                if tool_name in tools_by_name:
                    tool_args = json.loads(tool_call.function.arguments)
                    tool_to_call = tools_by_name[tool_name]
                    result = tool_to_call(**tool_args)
                    traj.messages_and_choices.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_name,
                            "content": str(result),
                        }  # type: ignore[attr-defined]
                    )

                    if tool_name == "return_final_answer":
                        traj.final_answer = result
                        return traj
        except Exception as e:
            print(f"Error parsing tool calls: {e}")
            return traj

    return traj


# @weave.op()
async def run_agent_and_score(
    model: art.Model, scenario: Scenario
) -> ProjectTrajectory:
    traj = await run_agent(model, scenario)
    if traj.final_answer is None:
        traj.reward = 0.0
        return traj
    correctness_judge_response = await judge_correctness(
        scenario, traj.final_answer.answer
    )
    traj.reward = float(correctness_judge_response.accept)
    return traj


if __name__ == "__main__":
    import asyncio
    from load_scenarios import load_scenarios

    scenario = load_scenarios(limit=1)[0]
    model = art.Model(name="openai/gpt-4.1", project="agent-class-art")
    answer = asyncio.run(run_agent_and_score(model, scenario))
    print(answer)
